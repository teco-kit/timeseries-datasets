{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsflex\n",
    "from dataloaders import data_dict, data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to load participants\n"
     ]
    }
   ],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "args_dict = {\n",
    "    \"ucihar\": {\n",
    "        \"root_path\": r\"../UCI HAR Dataset\",\n",
    "        \"freq_save_path\": r\"../freq_data\",\n",
    "        \"data_name\": \"ucihar\"\n",
    "    },\n",
    "    \"oppo\": {\n",
    "        \"root_path\": r\"../OpportunityUCIDataset/dataset\",\n",
    "        \"freq_save_path\": r\"../freq_data\",\n",
    "        \"data_name\": \"oppo\"\n",
    "    },\n",
    "    \"pamap2\": {\n",
    "        \"root_path\": r\"../PAMAP2_Dataset/Protocol/\",\n",
    "        \"freq_save_path\": r\"../freq_data\",\n",
    "        \"data_name\": \"pamap2\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "data_name = \"pamap2\"\n",
    "\n",
    "args = dotdict(args_dict[data_name])    \n",
    "\n",
    "args.difference = False \n",
    "args.sampling_freq =  100\n",
    "args.train_vali_quote = 0.95\n",
    "# for this dataset the window size is 128\n",
    "args.windowsize = int(2.56 * args.sampling_freq)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args.drop_long = False\n",
    "\n",
    "args.datanorm_type = \"standardization\" # None ,\"standardization\", \"minmax\"\n",
    "\n",
    "args.wavename = \"morl\"\n",
    "# if you want to use raw time series as input, set model_type as time\n",
    "# if you want to use spectrogram as input, set model_type as freq\n",
    "# if you want to use both as inputs, set model_type as cross\n",
    "args.model_type = \"time\"\n",
    "\n",
    "# if you want to do Leave one out cross validation, set exp_mode as \"LOCV\"\n",
    "# if you want to do the given train test experiment, set exp_mode as \"Given\"\n",
    "# if you want to do Semi non overlapping experiment, set exp_mode as \"SOCV\"\n",
    "# if you want to do Full non overlapping experiment, set exp_mode as \"FOCV\"\n",
    "args.exp_mode = \"Given\"\n",
    "\n",
    "if args.exp_mode == \"FOCV\":\n",
    "    args.displacement =  int(1 * args.windowsize) \n",
    "else:\n",
    "    args.displacement =  int(0.5 * args.windowsize)     \n",
    "\n",
    "\n",
    "args.batch_size = 32\n",
    "args.shuffle = True\n",
    "args.drop_last = False\n",
    "dataset = data_dict[args.data_name](args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                           time  acc_16_01_hand\n",
      "0      1970-01-01 00:00:00.000        -9.91204\n",
      "1      1970-01-01 00:00:00.010        -9.87531\n",
      "2      1970-01-01 00:00:00.020        -9.72175\n",
      "3      1970-01-01 00:00:00.030        -9.79920\n",
      "4      1970-01-01 00:00:00.040        -9.49956\n",
      "...                        ...             ...\n",
      "408025 1970-01-01 01:08:00.250        -9.54108\n",
      "408026 1970-01-01 01:08:00.260        -9.42932\n",
      "408027 1970-01-01 01:08:00.270        -9.42745\n",
      "408028 1970-01-01 01:08:00.280        -9.47246\n",
      "408029 1970-01-01 01:08:00.290        -9.66621\n",
      "\n",
      "[408030 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "sensors_0 = dataset.participants[0]\n",
    "data_0 = [v for k,v in sensors_0.items()]\n",
    "data_headers = [k for k,v in sensors_0.items()]\n",
    "\n",
    "print(data_0[0].head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/king/opt/anaconda3/lib/python3.9/site-packages/tsflex/features/feature_collection.py\", line 630, in calculate\n",
      "    calculated_feature_list = [self._executor(idx) for idx in idxs]\n",
      "  File \"/Users/king/opt/anaconda3/lib/python3.9/site-packages/tsflex/features/feature_collection.py\", line 630, in <listcomp>\n",
      "    calculated_feature_list = [self._executor(idx) for idx in idxs]\n",
      "  File \"/Users/king/opt/anaconda3/lib/python3.9/site-packages/tsflex/features/feature_collection.py\", line 265, in _executor\n",
      "    stroll, function = get_stroll_func(idx)\n",
      "  File \"/Users/king/opt/anaconda3/lib/python3.9/site-packages/tsflex/features/feature_collection.py\", line 314, in get_stroll_function\n",
      "    stroll = StridedRollingFactory.get_segmenter(**stroll_arg_dict)\n",
      "  File \"/Users/king/opt/anaconda3/lib/python3.9/site-packages/tsflex/features/segmenter/strided_rolling_factory.py\", line 80, in get_segmenter\n",
      "    raise ValueError(\"Cannot segment a sequence-series with a time window\")\n",
      "ValueError: Cannot segment a sequence-series with a time window\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Feature Extraction halted due to error while extracting one (or multiple) feature(s)! See stack trace above.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/king/timeseries-datasets/notebooks/toSeries.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/king/timeseries-datasets/notebooks/toSeries.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/king/timeseries-datasets/notebooks/toSeries.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m fc \u001b[39m=\u001b[39m FeatureCollection(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/king/timeseries-datasets/notebooks/toSeries.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     feature_descriptors\u001b[39m=\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/king/timeseries-datasets/notebooks/toSeries.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     FeatureDescriptor(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/king/timeseries-datasets/notebooks/toSeries.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/king/timeseries-datasets/notebooks/toSeries.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/king/timeseries-datasets/notebooks/toSeries.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m fc\u001b[39m.\u001b[39;49mcalculate(data\u001b[39m=\u001b[39;49mdata_0[\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tsflex/features/feature_collection.py:654\u001b[0m, in \u001b[0;36mFeatureCollection.calculate\u001b[0;34m(self, data, stride, segment_start_idxs, segment_end_idxs, return_df, window_idx, include_final_window, bound_method, approve_sparsity, show_progress, logging_file_path, n_jobs)\u001b[0m\n\u001b[1;32m    651\u001b[0m     logger\u001b[39m.\u001b[39mremoveHandler(f_handler)\n\u001b[1;32m    653\u001b[0m \u001b[39mif\u001b[39;00m calculated_feature_list \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    655\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFeature Extraction halted due to error while extracting one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    656\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m(or multiple) feature(s)! See stack trace above.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    657\u001b[0m     )\n\u001b[1;32m    659\u001b[0m \u001b[39mif\u001b[39;00m return_df:\n\u001b[1;32m    660\u001b[0m     \u001b[39m# concatenate & sort the columns\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(calculated_feature_list, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, join\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mouter\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Feature Extraction halted due to error while extracting one (or multiple) feature(s)! See stack trace above."
     ]
    }
   ],
   "source": [
    "from tsflex.features import FeatureCollection, FeatureDescriptor, FuncWrapper\n",
    "import numpy as np\n",
    "\n",
    "fc = FeatureCollection(\n",
    "    feature_descriptors=\n",
    "    FeatureDescriptor(\n",
    "        function=FuncWrapper(func=np.mean, output_names=\"np_means\"),\n",
    "        series_name=\"acc_16_01_hand\",\n",
    "        window=\"2.5s\", stride=\"1.25s\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fc.calculate(data=data_0[0].set_index(\"timestamp\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a1a00779b22bbdb22ae125091f6c7577bb6b9231d9b16ead7f7cdfb2262648d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
